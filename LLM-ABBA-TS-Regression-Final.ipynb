{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8306b5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "347ae35a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 16:19:09.229912: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-06 16:19:10.135694: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from fABBA import JABBA\n",
    "from fABBA import fABBA\n",
    "from fABBA import image_compress\n",
    "from fABBA import image_decompress\n",
    "import os\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, f1_score\n",
    "from src.preprocessing import encoders, vector_embed\n",
    "import torch\n",
    "import math\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "\n",
    "from utils.data_loader import load_from_tsfile_to_dataframe\n",
    "from utils.regressor_tools import process_data, fit_regressor, calculate_regression_metrics\n",
    "from utils.tools import create_directory\n",
    "from utils.transformer_tools import fit_transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31b7705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "300bdd8c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af060d7f",
   "metadata": {},
   "source": [
    "# Load data from the TS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3818ee2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "119it [00:03, 29.80it/s]\n",
      "66it [00:01, 33.31it/s] \n",
      "100%|██████████| 137/137 [00:00<00:00, 761.76it/s]\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Python program to convert a list to string\n",
    "def listToString(s):\n",
    "    # initialize an empty string\n",
    "    str1 = \"\"\n",
    "    # traverse in the string\n",
    "    for ele in s:\n",
    "        str1 += ele + ' '\n",
    "    # return string\n",
    "    return str1\n",
    "\n",
    "\n",
    "data_template = {\n",
    "    \"text\": [],\n",
    "    \"labels\": [],\n",
    "}\n",
    "\n",
    "###############   Time Series Data   ###############\n",
    "\n",
    "data_name = ['AppliancesEnergy', 'HouseholdPowerConsumption1', 'HouseholdPowerConsumption2', 'BenzeneConcentration',\n",
    "            'BeijingPM25Quality', 'BeijingPM10Quality', 'LiveFuelMoistureContent', 'FloodModeling1', 'FloodModeling2',\n",
    "            'FloodModeling3', 'AustraliaRainfall', 'PPGDalia', 'IEEEPPG', 'BIDMCRR', 'BIDMCHR', 'BIDMCSpO2', 'NewsHeadlineSentiment',\n",
    "            'NewsTitleSentiment', 'Covid3Month']\n",
    "\n",
    "model_name = 'roberta'\n",
    "data_folder = 'data/monash-regression/'\n",
    "train_file = data_folder + data_name[0] + \"_TRAIN.ts\"\n",
    "test_file = data_folder + data_name[0] + \"_TEST.ts\"\n",
    "    \n",
    "X_train, y_train = load_from_tsfile_to_dataframe(train_file)\n",
    "X_test, y_test = load_from_tsfile_to_dataframe(test_file)\n",
    "norm = \"minmax\"               # none, standard, minmax\n",
    "\n",
    "train_test_split = [X_train.shape[0], X_test.shape[0]]\n",
    "data_all = pd.concat([X_train, X_test])\n",
    "target_scaled = np.concatenate([y_train, y_test])\n",
    "\n",
    "\n",
    "\n",
    "# print(\"[{}] X_train: {}\".format(module, X_train.shape))\n",
    "# print(\"[{}] X_test: {}\".format(module, X_test.shape))\n",
    "\n",
    "# in case there are different lengths in the dataset, we need to consider that.\n",
    "# assume that all the dimensions are the same length\n",
    "# print(\"[{}] Finding minimum length\".format(module))\n",
    "min_len = np.inf\n",
    "for i in range(len(data_all)):\n",
    "    x = data_all.iloc[i, :]\n",
    "    all_len = [len(y) for y in x]\n",
    "    min_len = min(min(all_len), min_len)\n",
    "# print(\"[{}] Minimum length: {}\".format(module, min_len))\n",
    "\n",
    "# process the data into numpy array with (n_examples, n_timestep, n_dim)\n",
    "# print(\"[{}] Reshaping data\".format(module))\n",
    "data_scaled = process_data(data_all, normalise=norm, min_len=min_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f40fc1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_0</th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>dim_6</th>\n",
       "      <th>dim_7</th>\n",
       "      <th>dim_8</th>\n",
       "      <th>dim_9</th>\n",
       "      <th>...</th>\n",
       "      <th>dim_14</th>\n",
       "      <th>dim_15</th>\n",
       "      <th>dim_16</th>\n",
       "      <th>dim_17</th>\n",
       "      <th>dim_18</th>\n",
       "      <th>dim_19</th>\n",
       "      <th>dim_20</th>\n",
       "      <th>dim_21</th>\n",
       "      <th>dim_22</th>\n",
       "      <th>dim_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-02-28 17:00:00    21.29\n",
       "2016-02-28 17:10:...</td>\n",
       "      <td>2016-02-28 17:00:00    31.666667\n",
       "2016-02-28 17...</td>\n",
       "      <td>2016-02-28 17:00:00    19.890000\n",
       "2016-02-28 17...</td>\n",
       "      <td>2016-02-28 17:00:00    31.566667\n",
       "2016-02-28 17...</td>\n",
       "      <td>2016-02-28 17:00:00    20.200000\n",
       "2016-02-28 17...</td>\n",
       "      <td>2016-02-28 17:00:00    34.230000\n",
       "2016-02-28 17...</td>\n",
       "      <td>2016-02-28 17:00:00    19.730000\n",
       "2016-02-28 17...</td>\n",
       "      <td>2016-02-28 17:00:00    31.000000\n",
       "2016-02-28 17...</td>\n",
       "      <td>2016-02-28 17:00:00    17.390000\n",
       "2016-02-28 17...</td>\n",
       "      <td>2016-02-28 17:00:00    45.163333\n",
       "2016-02-28 17...</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-02-28 17:00:00    22.533333\n",
       "2016-02-28 17...</td>\n",
       "      <td>2016-02-28 17:00:00    34.590000\n",
       "2016-02-28 17...</td>\n",
       "      <td>2016-02-28 17:00:00    18.500000\n",
       "2016-02-28 17...</td>\n",
       "      <td>2016-02-28 17:00:00    39.863333\n",
       "2016-02-28 17...</td>\n",
       "      <td>2016-02-28 17:00:00    5.500000\n",
       "2016-02-28 17:...</td>\n",
       "      <td>2016-02-28 17:00:00    758.000000\n",
       "2016-02-28 1...</td>\n",
       "      <td>2016-02-28 17:00:00    50.000000\n",
       "2016-02-28 17...</td>\n",
       "      <td>2016-02-28 17:00:00    7.000000\n",
       "2016-02-28 17:...</td>\n",
       "      <td>2016-02-28 17:00:00    40.0\n",
       "2016-02-28 17:10:0...</td>\n",
       "      <td>2016-02-28 17:00:00   -4.200000\n",
       "2016-02-28 17:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-03-03 17:00:00    21.600000\n",
       "2016-03-03 17...</td>\n",
       "      <td>2016-03-03 17:00:00    39.110000\n",
       "2016-03-03 17...</td>\n",
       "      <td>2016-03-03 17:00:00    20.000000\n",
       "2016-03-03 17...</td>\n",
       "      <td>2016-03-03 17:00:00    37.420000\n",
       "2016-03-03 17...</td>\n",
       "      <td>2016-03-03 17:00:00    20.600000\n",
       "2016-03-03 17...</td>\n",
       "      <td>2016-03-03 17:00:00    37.411429\n",
       "2016-03-03 17...</td>\n",
       "      <td>2016-03-03 17:00:00    21.100000\n",
       "2016-03-03 17...</td>\n",
       "      <td>2016-03-03 17:00:00    34.556250\n",
       "2016-03-03 17...</td>\n",
       "      <td>2016-03-03 17:00:00    17.937143\n",
       "2016-03-03 17...</td>\n",
       "      <td>2016-03-03 17:00:00    44.621429\n",
       "2016-03-03 17...</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-03-03 17:00:00    21.968571\n",
       "2016-03-03 17...</td>\n",
       "      <td>2016-03-03 17:00:00    37.000000\n",
       "2016-03-03 17...</td>\n",
       "      <td>2016-03-03 17:00:00    18.700000\n",
       "2016-03-03 17...</td>\n",
       "      <td>2016-03-03 17:00:00    41.290000\n",
       "2016-03-03 17...</td>\n",
       "      <td>2016-03-03 17:00:00    7.000000\n",
       "2016-03-03 17:...</td>\n",
       "      <td>2016-03-03 17:00:00    749.200000\n",
       "2016-03-03 1...</td>\n",
       "      <td>2016-03-03 17:00:00    66.000000\n",
       "2016-03-03 17...</td>\n",
       "      <td>2016-03-03 17:00:00    4.000000\n",
       "2016-03-03 17:...</td>\n",
       "      <td>2016-03-03 17:00:00    40.0\n",
       "2016-03-03 17:10:0...</td>\n",
       "      <td>2016-03-03 17:00:00    1.100000\n",
       "2016-03-03 17:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-28 17:00:00    19.600000\n",
       "2016-01-28 17...</td>\n",
       "      <td>2016-01-28 17:00:00    45.090000\n",
       "2016-01-28 17...</td>\n",
       "      <td>2016-01-28 17:00:00    18.790000\n",
       "2016-01-28 17...</td>\n",
       "      <td>2016-01-28 17:00:00    44.200000\n",
       "2016-01-28 17...</td>\n",
       "      <td>2016-01-28 17:00:00    19.823333\n",
       "2016-01-28 17...</td>\n",
       "      <td>2016-01-28 17:00:00    44.626667\n",
       "2016-01-28 17...</td>\n",
       "      <td>2016-01-28 17:00:00    18.260000\n",
       "2016-01-28 17...</td>\n",
       "      <td>2016-01-28 17:00:00    44.026667\n",
       "2016-01-28 17...</td>\n",
       "      <td>2016-01-28 17:00:00    17.2\n",
       "2016-01-28 17:10:0...</td>\n",
       "      <td>2016-01-28 17:00:00    52.090000\n",
       "2016-01-28 17...</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-01-28 17:00:00    19.011111\n",
       "2016-01-28 17...</td>\n",
       "      <td>2016-01-28 17:00:00    45.102222\n",
       "2016-01-28 17...</td>\n",
       "      <td>2016-01-28 17:00:00    17.29\n",
       "2016-01-28 17:10:...</td>\n",
       "      <td>2016-01-28 17:00:00    46.560000\n",
       "2016-01-28 17...</td>\n",
       "      <td>2016-01-28 17:00:00    5.500000\n",
       "2016-01-28 17:...</td>\n",
       "      <td>2016-01-28 17:00:00    765.100000\n",
       "2016-01-28 1...</td>\n",
       "      <td>2016-01-28 17:00:00    77.000000\n",
       "2016-01-28 17...</td>\n",
       "      <td>2016-01-28 17:00:00     2.0\n",
       "2016-01-28 17:10:0...</td>\n",
       "      <td>2016-01-28 17:00:00    40.0\n",
       "2016-01-28 17:10:0...</td>\n",
       "      <td>2016-01-28 17:00:00    1.800000\n",
       "2016-01-28 17:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-03-12 17:00:00    20.600000\n",
       "2016-03-12 17...</td>\n",
       "      <td>2016-03-12 17:00:00    33.266667\n",
       "2016-03-12 17...</td>\n",
       "      <td>2016-03-12 17:00:00    19.200000\n",
       "2016-03-12 17...</td>\n",
       "      <td>2016-03-12 17:00:00    33.500000\n",
       "2016-03-12 17...</td>\n",
       "      <td>2016-03-12 17:00:00    19.89\n",
       "2016-03-12 17:10:...</td>\n",
       "      <td>2016-03-12 17:00:00    33.500000\n",
       "2016-03-12 17...</td>\n",
       "      <td>2016-03-12 17:00:00    20.500000\n",
       "2016-03-12 17...</td>\n",
       "      <td>2016-03-12 17:00:00    33.090000\n",
       "2016-03-12 17...</td>\n",
       "      <td>2016-03-12 17:00:00    18.633333\n",
       "2016-03-12 17...</td>\n",
       "      <td>2016-03-12 17:00:00    41.800000\n",
       "2016-03-12 17...</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-03-12 17:00:00    25.100000\n",
       "2016-03-12 17...</td>\n",
       "      <td>2016-03-12 17:00:00    33.290000\n",
       "2016-03-12 17...</td>\n",
       "      <td>2016-03-12 17:00:00    19.000000\n",
       "2016-03-12 17...</td>\n",
       "      <td>2016-03-12 17:00:00    39.200000\n",
       "2016-03-12 17...</td>\n",
       "      <td>2016-03-12 17:00:00    9.80\n",
       "2016-03-12 17:10:0...</td>\n",
       "      <td>2016-03-12 17:00:00    766.700000\n",
       "2016-03-12 1...</td>\n",
       "      <td>2016-03-12 17:00:00    50.000000\n",
       "2016-03-12 17...</td>\n",
       "      <td>2016-03-12 17:00:00    4.000000\n",
       "2016-03-12 17:...</td>\n",
       "      <td>2016-03-12 17:00:00    25.000000\n",
       "2016-03-12 17...</td>\n",
       "      <td>2016-03-12 17:00:00   -2.000000e-01\n",
       "2016-03-12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-03-14 17:00:00    22.20\n",
       "2016-03-14 17:10:...</td>\n",
       "      <td>2016-03-14 17:00:00    34.530000\n",
       "2016-03-14 17...</td>\n",
       "      <td>2016-03-14 17:00:00    20.000000\n",
       "2016-03-14 17...</td>\n",
       "      <td>2016-03-14 17:00:00    34.290000\n",
       "2016-03-14 17...</td>\n",
       "      <td>2016-03-14 17:00:00    23.963333\n",
       "2016-03-14 17...</td>\n",
       "      <td>2016-03-14 17:00:00    35.560000\n",
       "2016-03-14 17...</td>\n",
       "      <td>2016-03-14 17:00:00    21.000000\n",
       "2016-03-14 17...</td>\n",
       "      <td>2016-03-14 17:00:00    33.230000\n",
       "2016-03-14 17...</td>\n",
       "      <td>2016-03-14 17:00:00    18.200000\n",
       "2016-03-14 17...</td>\n",
       "      <td>2016-03-14 17:00:00    44.200000\n",
       "2016-03-14 17...</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-03-14 17:00:00    22.500000\n",
       "2016-03-14 17...</td>\n",
       "      <td>2016-03-14 17:00:00    30.000000\n",
       "2016-03-14 17...</td>\n",
       "      <td>2016-03-14 17:00:00    19.70\n",
       "2016-03-14 17:10:...</td>\n",
       "      <td>2016-03-14 17:00:00    37.70\n",
       "2016-03-14 17:10:...</td>\n",
       "      <td>2016-03-14 17:00:00     7.900000\n",
       "2016-03-14 17...</td>\n",
       "      <td>2016-03-14 17:00:00    766.700000\n",
       "2016-03-14 1...</td>\n",
       "      <td>2016-03-14 17:00:00    55.000000\n",
       "2016-03-14 17...</td>\n",
       "      <td>2016-03-14 17:00:00    6.000000\n",
       "2016-03-14 17:...</td>\n",
       "      <td>2016-03-14 17:00:00    25.000000\n",
       "2016-03-14 17...</td>\n",
       "      <td>2016-03-14 17:00:00   -0.600000\n",
       "2016-03-14 17:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>2016-02-03 17:00:00    21.926667\n",
       "2016-02-03 17...</td>\n",
       "      <td>2016-02-03 17:00:00    43.900000\n",
       "2016-02-03 17...</td>\n",
       "      <td>2016-02-03 17:00:00    20.700000\n",
       "2016-02-03 17...</td>\n",
       "      <td>2016-02-03 17:00:00    43.066667\n",
       "2016-02-03 17...</td>\n",
       "      <td>2016-02-03 17:00:00    23.116667\n",
       "2016-02-03 17...</td>\n",
       "      <td>2016-02-03 17:00:00    47.651667\n",
       "2016-02-03 17...</td>\n",
       "      <td>2016-02-03 17:00:00    21.390000\n",
       "2016-02-03 17...</td>\n",
       "      <td>2016-02-03 17:00:00    41.060000\n",
       "2016-02-03 17...</td>\n",
       "      <td>2016-02-03 17:00:00    19.000000\n",
       "2016-02-03 17...</td>\n",
       "      <td>2016-02-03 17:00:00    54.530000\n",
       "2016-02-03 17...</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-02-03 17:00:00    21.790000\n",
       "2016-02-03 17...</td>\n",
       "      <td>2016-02-03 17:00:00    44.202941\n",
       "2016-02-03 17...</td>\n",
       "      <td>2016-02-03 17:00:00    18.60\n",
       "2016-02-03 17:10:...</td>\n",
       "      <td>2016-02-03 17:00:00    44.760000\n",
       "2016-02-03 17...</td>\n",
       "      <td>2016-02-03 17:00:00     5.300000\n",
       "2016-02-03 17...</td>\n",
       "      <td>2016-02-03 17:00:00    761.800000\n",
       "2016-02-03 1...</td>\n",
       "      <td>2016-02-03 17:00:00    80.000000\n",
       "2016-02-03 17...</td>\n",
       "      <td>2016-02-03 17:00:00    6.000000\n",
       "2016-02-03 17:...</td>\n",
       "      <td>2016-02-03 17:00:00    40.0\n",
       "2016-02-03 17:10:0...</td>\n",
       "      <td>2016-02-03 17:00:00    2.100000\n",
       "2016-02-03 17:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2016-05-06 17:00:00    24.790000\n",
       "2016-05-06 17...</td>\n",
       "      <td>2016-05-06 17:00:00    35.500000\n",
       "2016-05-06 17...</td>\n",
       "      <td>2016-05-06 17:00:00    25.100000\n",
       "2016-05-06 17...</td>\n",
       "      <td>2016-05-06 17:00:00    32.193333\n",
       "2016-05-06 17...</td>\n",
       "      <td>2016-05-06 17:00:00    24.200\n",
       "2016-05-06 17:10...</td>\n",
       "      <td>2016-05-06 17:00:00    36.0000\n",
       "2016-05-06 17:1...</td>\n",
       "      <td>2016-05-06 17:00:00    23.963333\n",
       "2016-05-06 17...</td>\n",
       "      <td>2016-05-06 17:00:00    35.326667\n",
       "2016-05-06 17...</td>\n",
       "      <td>2016-05-06 17:00:00    20.600000\n",
       "2016-05-06 17...</td>\n",
       "      <td>2016-05-06 17:00:00    44.790000\n",
       "2016-05-06 17...</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-05-06 17:00:00    23.70\n",
       "2016-05-06 17:10:...</td>\n",
       "      <td>2016-05-06 17:00:00    37.933333\n",
       "2016-05-06 17...</td>\n",
       "      <td>2016-05-06 17:00:00    21.566667\n",
       "2016-05-06 17...</td>\n",
       "      <td>2016-05-06 17:00:00    38.933333\n",
       "2016-05-06 17...</td>\n",
       "      <td>2016-05-06 17:00:00    22.800000\n",
       "2016-05-06 17...</td>\n",
       "      <td>2016-05-06 17:00:00    752.700000\n",
       "2016-05-06 1...</td>\n",
       "      <td>2016-05-06 17:00:00    37.000000\n",
       "2016-05-06 17...</td>\n",
       "      <td>2016-05-06 17:00:00    5.0\n",
       "2016-05-06 17:10:00...</td>\n",
       "      <td>2016-05-06 17:00:00    40.0\n",
       "2016-05-06 17:10:0...</td>\n",
       "      <td>2016-05-06 17:00:00    7.50\n",
       "2016-05-06 17:10:0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>2016-03-04 17:00:00    21.000000\n",
       "2016-03-04 17...</td>\n",
       "      <td>2016-03-04 17:00:00    38.790000\n",
       "2016-03-04 17...</td>\n",
       "      <td>2016-03-04 17:00:00    19.685714\n",
       "2016-03-04 17...</td>\n",
       "      <td>2016-03-04 17:00:00    38.261429\n",
       "2016-03-04 17...</td>\n",
       "      <td>2016-03-04 17:00:00    22.777143\n",
       "2016-03-04 17...</td>\n",
       "      <td>2016-03-04 17:00:00    38.381429\n",
       "2016-03-04 17...</td>\n",
       "      <td>2016-03-04 17:00:00    19.1\n",
       "2016-03-04 17:10:0...</td>\n",
       "      <td>2016-03-04 17:00:00    35.900000\n",
       "2016-03-04 17...</td>\n",
       "      <td>2016-03-04 17:00:00    18.000000\n",
       "2016-03-04 17...</td>\n",
       "      <td>2016-03-04 17:00:00    46.200000\n",
       "2016-03-04 17...</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-03-04 17:00:00    20.166667\n",
       "2016-03-04 17...</td>\n",
       "      <td>2016-03-04 17:00:00    38.060000\n",
       "2016-03-04 17...</td>\n",
       "      <td>2016-03-04 17:00:00    18.500000\n",
       "2016-03-04 17...</td>\n",
       "      <td>2016-03-04 17:00:00    38.790000\n",
       "2016-03-04 17...</td>\n",
       "      <td>2016-03-04 17:00:00    7.500000\n",
       "2016-03-04 17:...</td>\n",
       "      <td>2016-03-04 17:00:00    739.200000\n",
       "2016-03-04 1...</td>\n",
       "      <td>2016-03-04 17:00:00    63.000000\n",
       "2016-03-04 17...</td>\n",
       "      <td>2016-03-04 17:00:00    1.000000\n",
       "2016-03-04 17:...</td>\n",
       "      <td>2016-03-04 17:00:00    40.000000\n",
       "2016-03-04 17...</td>\n",
       "      <td>2016-03-04 17:00:00    0.900000\n",
       "2016-03-04 17:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>2016-02-18 17:00:00    20.00\n",
       "2016-02-18 17:10:...</td>\n",
       "      <td>2016-02-18 17:00:00    33.966667\n",
       "2016-02-18 17...</td>\n",
       "      <td>2016-02-18 17:00:00    18.323333\n",
       "2016-02-18 17...</td>\n",
       "      <td>2016-02-18 17:00:00    34.900000\n",
       "2016-02-18 17...</td>\n",
       "      <td>2016-02-18 17:00:00    19.725714\n",
       "2016-02-18 17...</td>\n",
       "      <td>2016-02-18 17:00:00    36.290000\n",
       "2016-02-18 17...</td>\n",
       "      <td>2016-02-18 17:00:00    18.666667\n",
       "2016-02-18 17...</td>\n",
       "      <td>2016-02-18 17:00:00    33.933333\n",
       "2016-02-18 17...</td>\n",
       "      <td>2016-02-18 17:00:00    17.29\n",
       "2016-02-18 17:10:...</td>\n",
       "      <td>2016-02-18 17:00:00    42.790000\n",
       "2016-02-18 17...</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-02-18 17:00:00    19.500000\n",
       "2016-02-18 17...</td>\n",
       "      <td>2016-02-18 17:00:00    36.036000\n",
       "2016-02-18 17...</td>\n",
       "      <td>2016-02-18 17:00:00    17.600000\n",
       "2016-02-18 17...</td>\n",
       "      <td>2016-02-18 17:00:00    35.277143\n",
       "2016-02-18 17...</td>\n",
       "      <td>2016-02-18 17:00:00    3.100000\n",
       "2016-02-18 17:...</td>\n",
       "      <td>2016-02-18 17:00:00    756.200000\n",
       "2016-02-18 1...</td>\n",
       "      <td>2016-02-18 17:00:00    79.000000\n",
       "2016-02-18 17...</td>\n",
       "      <td>2016-02-18 17:00:00    2.0\n",
       "2016-02-18 17:10:00...</td>\n",
       "      <td>2016-02-18 17:00:00    29.000000\n",
       "2016-02-18 17...</td>\n",
       "      <td>2016-02-18 17:00:00   -0.300000\n",
       "2016-02-18 17:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2016-02-27 17:00:00    21.025000\n",
       "2016-02-27 17...</td>\n",
       "      <td>2016-02-27 17:00:00    35.175000\n",
       "2016-02-27 17...</td>\n",
       "      <td>2016-02-27 17:00:00    19.790000\n",
       "2016-02-27 17...</td>\n",
       "      <td>2016-02-27 17:00:00    34.500000\n",
       "2016-02-27 17...</td>\n",
       "      <td>2016-02-27 17:00:00    21.033333\n",
       "2016-02-27 17...</td>\n",
       "      <td>2016-02-27 17:00:00    35.500000\n",
       "2016-02-27 17...</td>\n",
       "      <td>2016-02-27 17:00:00    20.290000\n",
       "2016-02-27 17...</td>\n",
       "      <td>2016-02-27 17:00:00    32.633333\n",
       "2016-02-27 17...</td>\n",
       "      <td>2016-02-27 17:00:00    18.000000\n",
       "2016-02-27 17...</td>\n",
       "      <td>2016-02-27 17:00:00    45.360000\n",
       "2016-02-27 17...</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-02-27 17:00:00    22.700000\n",
       "2016-02-27 17...</td>\n",
       "      <td>2016-02-27 17:00:00    33.900000\n",
       "2016-02-27 17...</td>\n",
       "      <td>2016-02-27 17:00:00    18.200000\n",
       "2016-02-27 17...</td>\n",
       "      <td>2016-02-27 17:00:00    39.79\n",
       "2016-02-27 17:10:...</td>\n",
       "      <td>2016-02-27 17:00:00    6.00\n",
       "2016-02-27 17:10:0...</td>\n",
       "      <td>2016-02-27 17:00:00    751.300000\n",
       "2016-02-27 1...</td>\n",
       "      <td>2016-02-27 17:00:00    52.0\n",
       "2016-02-27 17:10:0...</td>\n",
       "      <td>2016-02-27 17:00:00    6.000000\n",
       "2016-02-27 17:...</td>\n",
       "      <td>2016-02-27 17:00:00    29.0\n",
       "2016-02-27 17:10:0...</td>\n",
       "      <td>2016-02-27 17:00:00   -3.200000\n",
       "2016-02-27 17:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                dim_0  \\\n",
       "0   2016-02-28 17:00:00    21.29\n",
       "2016-02-28 17:10:...   \n",
       "1   2016-03-03 17:00:00    21.600000\n",
       "2016-03-03 17...   \n",
       "2   2016-01-28 17:00:00    19.600000\n",
       "2016-01-28 17...   \n",
       "3   2016-03-12 17:00:00    20.600000\n",
       "2016-03-12 17...   \n",
       "4   2016-03-14 17:00:00    22.20\n",
       "2016-03-14 17:10:...   \n",
       "..                                                ...   \n",
       "90  2016-02-03 17:00:00    21.926667\n",
       "2016-02-03 17...   \n",
       "91  2016-05-06 17:00:00    24.790000\n",
       "2016-05-06 17...   \n",
       "92  2016-03-04 17:00:00    21.000000\n",
       "2016-03-04 17...   \n",
       "93  2016-02-18 17:00:00    20.00\n",
       "2016-02-18 17:10:...   \n",
       "94  2016-02-27 17:00:00    21.025000\n",
       "2016-02-27 17...   \n",
       "\n",
       "                                                dim_1  \\\n",
       "0   2016-02-28 17:00:00    31.666667\n",
       "2016-02-28 17...   \n",
       "1   2016-03-03 17:00:00    39.110000\n",
       "2016-03-03 17...   \n",
       "2   2016-01-28 17:00:00    45.090000\n",
       "2016-01-28 17...   \n",
       "3   2016-03-12 17:00:00    33.266667\n",
       "2016-03-12 17...   \n",
       "4   2016-03-14 17:00:00    34.530000\n",
       "2016-03-14 17...   \n",
       "..                                                ...   \n",
       "90  2016-02-03 17:00:00    43.900000\n",
       "2016-02-03 17...   \n",
       "91  2016-05-06 17:00:00    35.500000\n",
       "2016-05-06 17...   \n",
       "92  2016-03-04 17:00:00    38.790000\n",
       "2016-03-04 17...   \n",
       "93  2016-02-18 17:00:00    33.966667\n",
       "2016-02-18 17...   \n",
       "94  2016-02-27 17:00:00    35.175000\n",
       "2016-02-27 17...   \n",
       "\n",
       "                                                dim_2  \\\n",
       "0   2016-02-28 17:00:00    19.890000\n",
       "2016-02-28 17...   \n",
       "1   2016-03-03 17:00:00    20.000000\n",
       "2016-03-03 17...   \n",
       "2   2016-01-28 17:00:00    18.790000\n",
       "2016-01-28 17...   \n",
       "3   2016-03-12 17:00:00    19.200000\n",
       "2016-03-12 17...   \n",
       "4   2016-03-14 17:00:00    20.000000\n",
       "2016-03-14 17...   \n",
       "..                                                ...   \n",
       "90  2016-02-03 17:00:00    20.700000\n",
       "2016-02-03 17...   \n",
       "91  2016-05-06 17:00:00    25.100000\n",
       "2016-05-06 17...   \n",
       "92  2016-03-04 17:00:00    19.685714\n",
       "2016-03-04 17...   \n",
       "93  2016-02-18 17:00:00    18.323333\n",
       "2016-02-18 17...   \n",
       "94  2016-02-27 17:00:00    19.790000\n",
       "2016-02-27 17...   \n",
       "\n",
       "                                                dim_3  \\\n",
       "0   2016-02-28 17:00:00    31.566667\n",
       "2016-02-28 17...   \n",
       "1   2016-03-03 17:00:00    37.420000\n",
       "2016-03-03 17...   \n",
       "2   2016-01-28 17:00:00    44.200000\n",
       "2016-01-28 17...   \n",
       "3   2016-03-12 17:00:00    33.500000\n",
       "2016-03-12 17...   \n",
       "4   2016-03-14 17:00:00    34.290000\n",
       "2016-03-14 17...   \n",
       "..                                                ...   \n",
       "90  2016-02-03 17:00:00    43.066667\n",
       "2016-02-03 17...   \n",
       "91  2016-05-06 17:00:00    32.193333\n",
       "2016-05-06 17...   \n",
       "92  2016-03-04 17:00:00    38.261429\n",
       "2016-03-04 17...   \n",
       "93  2016-02-18 17:00:00    34.900000\n",
       "2016-02-18 17...   \n",
       "94  2016-02-27 17:00:00    34.500000\n",
       "2016-02-27 17...   \n",
       "\n",
       "                                                dim_4  \\\n",
       "0   2016-02-28 17:00:00    20.200000\n",
       "2016-02-28 17...   \n",
       "1   2016-03-03 17:00:00    20.600000\n",
       "2016-03-03 17...   \n",
       "2   2016-01-28 17:00:00    19.823333\n",
       "2016-01-28 17...   \n",
       "3   2016-03-12 17:00:00    19.89\n",
       "2016-03-12 17:10:...   \n",
       "4   2016-03-14 17:00:00    23.963333\n",
       "2016-03-14 17...   \n",
       "..                                                ...   \n",
       "90  2016-02-03 17:00:00    23.116667\n",
       "2016-02-03 17...   \n",
       "91  2016-05-06 17:00:00    24.200\n",
       "2016-05-06 17:10...   \n",
       "92  2016-03-04 17:00:00    22.777143\n",
       "2016-03-04 17...   \n",
       "93  2016-02-18 17:00:00    19.725714\n",
       "2016-02-18 17...   \n",
       "94  2016-02-27 17:00:00    21.033333\n",
       "2016-02-27 17...   \n",
       "\n",
       "                                                dim_5  \\\n",
       "0   2016-02-28 17:00:00    34.230000\n",
       "2016-02-28 17...   \n",
       "1   2016-03-03 17:00:00    37.411429\n",
       "2016-03-03 17...   \n",
       "2   2016-01-28 17:00:00    44.626667\n",
       "2016-01-28 17...   \n",
       "3   2016-03-12 17:00:00    33.500000\n",
       "2016-03-12 17...   \n",
       "4   2016-03-14 17:00:00    35.560000\n",
       "2016-03-14 17...   \n",
       "..                                                ...   \n",
       "90  2016-02-03 17:00:00    47.651667\n",
       "2016-02-03 17...   \n",
       "91  2016-05-06 17:00:00    36.0000\n",
       "2016-05-06 17:1...   \n",
       "92  2016-03-04 17:00:00    38.381429\n",
       "2016-03-04 17...   \n",
       "93  2016-02-18 17:00:00    36.290000\n",
       "2016-02-18 17...   \n",
       "94  2016-02-27 17:00:00    35.500000\n",
       "2016-02-27 17...   \n",
       "\n",
       "                                                dim_6  \\\n",
       "0   2016-02-28 17:00:00    19.730000\n",
       "2016-02-28 17...   \n",
       "1   2016-03-03 17:00:00    21.100000\n",
       "2016-03-03 17...   \n",
       "2   2016-01-28 17:00:00    18.260000\n",
       "2016-01-28 17...   \n",
       "3   2016-03-12 17:00:00    20.500000\n",
       "2016-03-12 17...   \n",
       "4   2016-03-14 17:00:00    21.000000\n",
       "2016-03-14 17...   \n",
       "..                                                ...   \n",
       "90  2016-02-03 17:00:00    21.390000\n",
       "2016-02-03 17...   \n",
       "91  2016-05-06 17:00:00    23.963333\n",
       "2016-05-06 17...   \n",
       "92  2016-03-04 17:00:00    19.1\n",
       "2016-03-04 17:10:0...   \n",
       "93  2016-02-18 17:00:00    18.666667\n",
       "2016-02-18 17...   \n",
       "94  2016-02-27 17:00:00    20.290000\n",
       "2016-02-27 17...   \n",
       "\n",
       "                                                dim_7  \\\n",
       "0   2016-02-28 17:00:00    31.000000\n",
       "2016-02-28 17...   \n",
       "1   2016-03-03 17:00:00    34.556250\n",
       "2016-03-03 17...   \n",
       "2   2016-01-28 17:00:00    44.026667\n",
       "2016-01-28 17...   \n",
       "3   2016-03-12 17:00:00    33.090000\n",
       "2016-03-12 17...   \n",
       "4   2016-03-14 17:00:00    33.230000\n",
       "2016-03-14 17...   \n",
       "..                                                ...   \n",
       "90  2016-02-03 17:00:00    41.060000\n",
       "2016-02-03 17...   \n",
       "91  2016-05-06 17:00:00    35.326667\n",
       "2016-05-06 17...   \n",
       "92  2016-03-04 17:00:00    35.900000\n",
       "2016-03-04 17...   \n",
       "93  2016-02-18 17:00:00    33.933333\n",
       "2016-02-18 17...   \n",
       "94  2016-02-27 17:00:00    32.633333\n",
       "2016-02-27 17...   \n",
       "\n",
       "                                                dim_8  \\\n",
       "0   2016-02-28 17:00:00    17.390000\n",
       "2016-02-28 17...   \n",
       "1   2016-03-03 17:00:00    17.937143\n",
       "2016-03-03 17...   \n",
       "2   2016-01-28 17:00:00    17.2\n",
       "2016-01-28 17:10:0...   \n",
       "3   2016-03-12 17:00:00    18.633333\n",
       "2016-03-12 17...   \n",
       "4   2016-03-14 17:00:00    18.200000\n",
       "2016-03-14 17...   \n",
       "..                                                ...   \n",
       "90  2016-02-03 17:00:00    19.000000\n",
       "2016-02-03 17...   \n",
       "91  2016-05-06 17:00:00    20.600000\n",
       "2016-05-06 17...   \n",
       "92  2016-03-04 17:00:00    18.000000\n",
       "2016-03-04 17...   \n",
       "93  2016-02-18 17:00:00    17.29\n",
       "2016-02-18 17:10:...   \n",
       "94  2016-02-27 17:00:00    18.000000\n",
       "2016-02-27 17...   \n",
       "\n",
       "                                                dim_9  ...  \\\n",
       "0   2016-02-28 17:00:00    45.163333\n",
       "2016-02-28 17...  ...   \n",
       "1   2016-03-03 17:00:00    44.621429\n",
       "2016-03-03 17...  ...   \n",
       "2   2016-01-28 17:00:00    52.090000\n",
       "2016-01-28 17...  ...   \n",
       "3   2016-03-12 17:00:00    41.800000\n",
       "2016-03-12 17...  ...   \n",
       "4   2016-03-14 17:00:00    44.200000\n",
       "2016-03-14 17...  ...   \n",
       "..                                                ...  ...   \n",
       "90  2016-02-03 17:00:00    54.530000\n",
       "2016-02-03 17...  ...   \n",
       "91  2016-05-06 17:00:00    44.790000\n",
       "2016-05-06 17...  ...   \n",
       "92  2016-03-04 17:00:00    46.200000\n",
       "2016-03-04 17...  ...   \n",
       "93  2016-02-18 17:00:00    42.790000\n",
       "2016-02-18 17...  ...   \n",
       "94  2016-02-27 17:00:00    45.360000\n",
       "2016-02-27 17...  ...   \n",
       "\n",
       "                                               dim_14  \\\n",
       "0   2016-02-28 17:00:00    22.533333\n",
       "2016-02-28 17...   \n",
       "1   2016-03-03 17:00:00    21.968571\n",
       "2016-03-03 17...   \n",
       "2   2016-01-28 17:00:00    19.011111\n",
       "2016-01-28 17...   \n",
       "3   2016-03-12 17:00:00    25.100000\n",
       "2016-03-12 17...   \n",
       "4   2016-03-14 17:00:00    22.500000\n",
       "2016-03-14 17...   \n",
       "..                                                ...   \n",
       "90  2016-02-03 17:00:00    21.790000\n",
       "2016-02-03 17...   \n",
       "91  2016-05-06 17:00:00    23.70\n",
       "2016-05-06 17:10:...   \n",
       "92  2016-03-04 17:00:00    20.166667\n",
       "2016-03-04 17...   \n",
       "93  2016-02-18 17:00:00    19.500000\n",
       "2016-02-18 17...   \n",
       "94  2016-02-27 17:00:00    22.700000\n",
       "2016-02-27 17...   \n",
       "\n",
       "                                               dim_15  \\\n",
       "0   2016-02-28 17:00:00    34.590000\n",
       "2016-02-28 17...   \n",
       "1   2016-03-03 17:00:00    37.000000\n",
       "2016-03-03 17...   \n",
       "2   2016-01-28 17:00:00    45.102222\n",
       "2016-01-28 17...   \n",
       "3   2016-03-12 17:00:00    33.290000\n",
       "2016-03-12 17...   \n",
       "4   2016-03-14 17:00:00    30.000000\n",
       "2016-03-14 17...   \n",
       "..                                                ...   \n",
       "90  2016-02-03 17:00:00    44.202941\n",
       "2016-02-03 17...   \n",
       "91  2016-05-06 17:00:00    37.933333\n",
       "2016-05-06 17...   \n",
       "92  2016-03-04 17:00:00    38.060000\n",
       "2016-03-04 17...   \n",
       "93  2016-02-18 17:00:00    36.036000\n",
       "2016-02-18 17...   \n",
       "94  2016-02-27 17:00:00    33.900000\n",
       "2016-02-27 17...   \n",
       "\n",
       "                                               dim_16  \\\n",
       "0   2016-02-28 17:00:00    18.500000\n",
       "2016-02-28 17...   \n",
       "1   2016-03-03 17:00:00    18.700000\n",
       "2016-03-03 17...   \n",
       "2   2016-01-28 17:00:00    17.29\n",
       "2016-01-28 17:10:...   \n",
       "3   2016-03-12 17:00:00    19.000000\n",
       "2016-03-12 17...   \n",
       "4   2016-03-14 17:00:00    19.70\n",
       "2016-03-14 17:10:...   \n",
       "..                                                ...   \n",
       "90  2016-02-03 17:00:00    18.60\n",
       "2016-02-03 17:10:...   \n",
       "91  2016-05-06 17:00:00    21.566667\n",
       "2016-05-06 17...   \n",
       "92  2016-03-04 17:00:00    18.500000\n",
       "2016-03-04 17...   \n",
       "93  2016-02-18 17:00:00    17.600000\n",
       "2016-02-18 17...   \n",
       "94  2016-02-27 17:00:00    18.200000\n",
       "2016-02-27 17...   \n",
       "\n",
       "                                               dim_17  \\\n",
       "0   2016-02-28 17:00:00    39.863333\n",
       "2016-02-28 17...   \n",
       "1   2016-03-03 17:00:00    41.290000\n",
       "2016-03-03 17...   \n",
       "2   2016-01-28 17:00:00    46.560000\n",
       "2016-01-28 17...   \n",
       "3   2016-03-12 17:00:00    39.200000\n",
       "2016-03-12 17...   \n",
       "4   2016-03-14 17:00:00    37.70\n",
       "2016-03-14 17:10:...   \n",
       "..                                                ...   \n",
       "90  2016-02-03 17:00:00    44.760000\n",
       "2016-02-03 17...   \n",
       "91  2016-05-06 17:00:00    38.933333\n",
       "2016-05-06 17...   \n",
       "92  2016-03-04 17:00:00    38.790000\n",
       "2016-03-04 17...   \n",
       "93  2016-02-18 17:00:00    35.277143\n",
       "2016-02-18 17...   \n",
       "94  2016-02-27 17:00:00    39.79\n",
       "2016-02-27 17:10:...   \n",
       "\n",
       "                                               dim_18  \\\n",
       "0   2016-02-28 17:00:00    5.500000\n",
       "2016-02-28 17:...   \n",
       "1   2016-03-03 17:00:00    7.000000\n",
       "2016-03-03 17:...   \n",
       "2   2016-01-28 17:00:00    5.500000\n",
       "2016-01-28 17:...   \n",
       "3   2016-03-12 17:00:00    9.80\n",
       "2016-03-12 17:10:0...   \n",
       "4   2016-03-14 17:00:00     7.900000\n",
       "2016-03-14 17...   \n",
       "..                                                ...   \n",
       "90  2016-02-03 17:00:00     5.300000\n",
       "2016-02-03 17...   \n",
       "91  2016-05-06 17:00:00    22.800000\n",
       "2016-05-06 17...   \n",
       "92  2016-03-04 17:00:00    7.500000\n",
       "2016-03-04 17:...   \n",
       "93  2016-02-18 17:00:00    3.100000\n",
       "2016-02-18 17:...   \n",
       "94  2016-02-27 17:00:00    6.00\n",
       "2016-02-27 17:10:0...   \n",
       "\n",
       "                                               dim_19  \\\n",
       "0   2016-02-28 17:00:00    758.000000\n",
       "2016-02-28 1...   \n",
       "1   2016-03-03 17:00:00    749.200000\n",
       "2016-03-03 1...   \n",
       "2   2016-01-28 17:00:00    765.100000\n",
       "2016-01-28 1...   \n",
       "3   2016-03-12 17:00:00    766.700000\n",
       "2016-03-12 1...   \n",
       "4   2016-03-14 17:00:00    766.700000\n",
       "2016-03-14 1...   \n",
       "..                                                ...   \n",
       "90  2016-02-03 17:00:00    761.800000\n",
       "2016-02-03 1...   \n",
       "91  2016-05-06 17:00:00    752.700000\n",
       "2016-05-06 1...   \n",
       "92  2016-03-04 17:00:00    739.200000\n",
       "2016-03-04 1...   \n",
       "93  2016-02-18 17:00:00    756.200000\n",
       "2016-02-18 1...   \n",
       "94  2016-02-27 17:00:00    751.300000\n",
       "2016-02-27 1...   \n",
       "\n",
       "                                               dim_20  \\\n",
       "0   2016-02-28 17:00:00    50.000000\n",
       "2016-02-28 17...   \n",
       "1   2016-03-03 17:00:00    66.000000\n",
       "2016-03-03 17...   \n",
       "2   2016-01-28 17:00:00    77.000000\n",
       "2016-01-28 17...   \n",
       "3   2016-03-12 17:00:00    50.000000\n",
       "2016-03-12 17...   \n",
       "4   2016-03-14 17:00:00    55.000000\n",
       "2016-03-14 17...   \n",
       "..                                                ...   \n",
       "90  2016-02-03 17:00:00    80.000000\n",
       "2016-02-03 17...   \n",
       "91  2016-05-06 17:00:00    37.000000\n",
       "2016-05-06 17...   \n",
       "92  2016-03-04 17:00:00    63.000000\n",
       "2016-03-04 17...   \n",
       "93  2016-02-18 17:00:00    79.000000\n",
       "2016-02-18 17...   \n",
       "94  2016-02-27 17:00:00    52.0\n",
       "2016-02-27 17:10:0...   \n",
       "\n",
       "                                               dim_21  \\\n",
       "0   2016-02-28 17:00:00    7.000000\n",
       "2016-02-28 17:...   \n",
       "1   2016-03-03 17:00:00    4.000000\n",
       "2016-03-03 17:...   \n",
       "2   2016-01-28 17:00:00     2.0\n",
       "2016-01-28 17:10:0...   \n",
       "3   2016-03-12 17:00:00    4.000000\n",
       "2016-03-12 17:...   \n",
       "4   2016-03-14 17:00:00    6.000000\n",
       "2016-03-14 17:...   \n",
       "..                                                ...   \n",
       "90  2016-02-03 17:00:00    6.000000\n",
       "2016-02-03 17:...   \n",
       "91  2016-05-06 17:00:00    5.0\n",
       "2016-05-06 17:10:00...   \n",
       "92  2016-03-04 17:00:00    1.000000\n",
       "2016-03-04 17:...   \n",
       "93  2016-02-18 17:00:00    2.0\n",
       "2016-02-18 17:10:00...   \n",
       "94  2016-02-27 17:00:00    6.000000\n",
       "2016-02-27 17:...   \n",
       "\n",
       "                                               dim_22  \\\n",
       "0   2016-02-28 17:00:00    40.0\n",
       "2016-02-28 17:10:0...   \n",
       "1   2016-03-03 17:00:00    40.0\n",
       "2016-03-03 17:10:0...   \n",
       "2   2016-01-28 17:00:00    40.0\n",
       "2016-01-28 17:10:0...   \n",
       "3   2016-03-12 17:00:00    25.000000\n",
       "2016-03-12 17...   \n",
       "4   2016-03-14 17:00:00    25.000000\n",
       "2016-03-14 17...   \n",
       "..                                                ...   \n",
       "90  2016-02-03 17:00:00    40.0\n",
       "2016-02-03 17:10:0...   \n",
       "91  2016-05-06 17:00:00    40.0\n",
       "2016-05-06 17:10:0...   \n",
       "92  2016-03-04 17:00:00    40.000000\n",
       "2016-03-04 17...   \n",
       "93  2016-02-18 17:00:00    29.000000\n",
       "2016-02-18 17...   \n",
       "94  2016-02-27 17:00:00    29.0\n",
       "2016-02-27 17:10:0...   \n",
       "\n",
       "                                               dim_23  \n",
       "0   2016-02-28 17:00:00   -4.200000\n",
       "2016-02-28 17:...  \n",
       "1   2016-03-03 17:00:00    1.100000\n",
       "2016-03-03 17:...  \n",
       "2   2016-01-28 17:00:00    1.800000\n",
       "2016-01-28 17:...  \n",
       "3   2016-03-12 17:00:00   -2.000000e-01\n",
       "2016-03-12...  \n",
       "4   2016-03-14 17:00:00   -0.600000\n",
       "2016-03-14 17:...  \n",
       "..                                                ...  \n",
       "90  2016-02-03 17:00:00    2.100000\n",
       "2016-02-03 17:...  \n",
       "91  2016-05-06 17:00:00    7.50\n",
       "2016-05-06 17:10:0...  \n",
       "92  2016-03-04 17:00:00    0.900000\n",
       "2016-03-04 17:...  \n",
       "93  2016-02-18 17:00:00   -0.300000\n",
       "2016-02-18 17:...  \n",
       "94  2016-02-27 17:00:00   -3.200000\n",
       "2016-02-27 17:...  \n",
       "\n",
       "[95 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a3d14a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'symbols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43msymbols\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'symbols' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f6ddb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "The length of used symbols is:1620\n"
     ]
    }
   ],
   "source": [
    "## init kmeans or agg\n",
    "## tol： compress rate, lower is better\n",
    "## k： bigger is better\n",
    "## scl： vibration rate\n",
    "## verbose： info\n",
    "qabba = JABBA(tol=0.005, init='agg', alpha=0.01, scl=3, verbose=0)\n",
    "# qabba = JABBA(tol=opts.ABBA_tol, init=opts.ABBA_init, k=opts.ABBA_k, scl=opts.ABBA_scl, verbose=0)\n",
    "symbols = qabba.fit_transform(data_scaled)\n",
    "# reconst = qabba.inverse_transform(symbols)  # set a shreshold that can optimize the generation\n",
    "symbols_convert = []\n",
    "print('##############################################################')\n",
    "print(\"The length of used symbols is:\" + str(qabba.parameters.centers.shape[0]))\n",
    "\n",
    "for i_data in range(len(symbols)):\n",
    "    symbols_convert.append(listToString(list(symbols[i_data])))\n",
    "\n",
    "train_data_symbolic, test_data_symbolic, train_target_symbolic, test_target_symbolic = symbols_convert[:train_test_split[0]], symbols_convert[train_test_split[0]:], target_scaled[:train_test_split[0]], target_scaled[train_test_split[0]:]\n",
    "\n",
    "data_TS = DatasetDict({\n",
    "    'train': Dataset.from_dict({'labels': train_target_symbolic, 'text': train_data_symbolic}),\n",
    "    'test': Dataset.from_dict({'labels': test_target_symbolic, 'text': test_data_symbolic})\n",
    "})\n",
    "\n",
    "# data_CIFAR.save_to_disk(f_hf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c886a0ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    " \n",
    "# Plotting a basic histogram\n",
    "plt.hist(target_scaled, bins=100, color='skyblue', edgecolor='black')\n",
    " \n",
    "# Adding labels and title\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Basic Histogram')\n",
    " \n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b626d9f0",
   "metadata": {},
   "source": [
    "# Prepare Common LLMs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beb2c096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49996cfe0f9c4499998fce37c92f582e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at starmpcc/Asclepius-Llama2-7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 159,911,936 || all params: 6,767,259,648 || trainable%: 2.3630\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "global model_tokenizer\n",
    "global batch_size\n",
    "global MAX_LENGTH\n",
    "global num_classes\n",
    "\n",
    "MAX_LENGTH = 4096\n",
    "num_classes = 1\n",
    "batch_size = 4\n",
    "# lora config\n",
    "\n",
    "\n",
    "model_checkpoint = \"starmpcc/Asclepius-Llama2-7B\"\n",
    "model_tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,\n",
    "                                                model_max_length=MAX_LENGTH,\n",
    "                                                padding_side=\"left\",\n",
    "                                                add_eos_token=True,\n",
    "                                                add_prefix_space=True)\n",
    "model_tokenizer.pad_token_id = model_tokenizer.eos_token_id\n",
    "model_tokenizer.pad_token = model_tokenizer.eos_token\n",
    "\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    # Load the model with 4-bit quantization\n",
    "    load_in_4bit=True,\n",
    "    # Use double quantization\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    # Use 4-bit Normal Float for storing the base model weights in GPU memory\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    # De-quantize the weights to 16-bit (Brain) float before the forward/backward pass\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model_input = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_checkpoint,\n",
    "    quantization_config=quantization_config,\n",
    "    num_labels=num_classes,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model_input = prepare_model_for_kbit_training(model_input)\n",
    "\n",
    "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
    "model_data_collator = DataCollatorWithPadding(tokenizer=model_tokenizer)\n",
    "\n",
    "model_input.config.pad_token_id = model_input.config.eos_token_id\n",
    "\n",
    "llama_peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    ")\n",
    "model_input = get_peft_model(model_input, llama_peft_config)\n",
    "\n",
    "model_input.print_trainable_parameters()\n",
    "model_input = model_input.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9934fd59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d941f77c69a47a1a37813f5d9321604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/95 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c723313466eb47d4b8bea084a76c8d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def model_preprocessing_function(examples):\n",
    "    return model_tokenizer(examples['text'], truncation=True, padding='max_length', max_length=MAX_LENGTH)\n",
    "\n",
    "\n",
    "model_tokenized_datasets = data_TS.map(model_preprocessing_function, batched=True)\n",
    "model_tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2544b6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Define the custom collate function\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_masks = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    # Pad or truncate sequences to the same length within each batch\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True)\n",
    "    attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True)\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    return {\n",
    "    'input_ids': input_ids,\n",
    "    'attention_mask': attention_masks,\n",
    "    'labels': labels\n",
    "    }\n",
    "\n",
    "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
    "# model_data_collator = DataCollatorWithPadding(tokenizer=model_tokenizer)\n",
    "data_loader = DataLoader(model_tokenized_datasets, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa3750cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MSELossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # Get model's predictions\n",
    "        outputs = model(**inputs)      \n",
    "        logits = outputs.get(\"logits\")\n",
    "   \n",
    "        # Compute MSE loss\n",
    "#         loss = F.mse_loss(logits.squeeze(), labels.float())\n",
    "        loss = F.mse_loss(logits.squeeze(), labels.float())/batch_size\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cae8f4",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model_data_collator = DataCollatorWithPadding(tokenizer=model_tokenizer)\n",
    "\n",
    "project = \"ts-finetune-\" + data_name[0]\n",
    "#b-instruct-v0.1-h\n",
    "base_model_name = \"llama2\" \n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=5e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    max_grad_norm=0.3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=10,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\",\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer_abba = MSELossTrainer(\n",
    "    model=model_input,\n",
    "    args=training_args,\n",
    "    train_dataset=model_tokenized_datasets['train'],\n",
    "    eval_dataset=model_tokenized_datasets[\"test\"],\n",
    "    data_collator=model_data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer_abba.train()\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "del model_input\n",
    "# del model_tokenized_datasets\n",
    "# del data_UCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c0a2ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7c9b12786c4962af031ac0be957852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at starmpcc/Asclepius-Llama2-7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "WARNING:accelerate.big_modeling:You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "You can't move a model that has some modules offloaded to cpu or disk.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 21\u001B[0m\n\u001B[1;32m      9\u001B[0m base_model \u001B[38;5;241m=\u001B[39m AutoModelForSequenceClassification\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m     10\u001B[0m     pretrained_model_name_or_path\u001B[38;5;241m=\u001B[39mbase_model_id,\n\u001B[1;32m     11\u001B[0m     num_labels\u001B[38;5;241m=\u001B[39mnum_classes,\n\u001B[1;32m     12\u001B[0m     device_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     13\u001B[0m )\n\u001B[1;32m     15\u001B[0m eval_tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m     16\u001B[0m     base_model_id,\n\u001B[1;32m     17\u001B[0m     add_bos_token\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     18\u001B[0m     trust_remote_code\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     19\u001B[0m )\n\u001B[0;32m---> 21\u001B[0m base_model \u001B[38;5;241m=\u001B[39m \u001B[43mbase_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/accelerate/big_modeling.py:455\u001B[0m, in \u001B[0;36mdispatch_model.<locals>.add_warning.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    453\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m param \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mparameters():\n\u001B[1;32m    454\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m param\u001B[38;5;241m.\u001B[39mdevice \u001B[38;5;241m==\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmeta\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 455\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou can\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt move a model that has some modules offloaded to cpu or disk.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    456\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: You can't move a model that has some modules offloaded to cpu or disk."
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "del model_input\n",
    "num_classes = 1\n",
    "base_model_id = \"starmpcc/Asclepius-Llama2-7B\"\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=base_model_id,\n",
    "    num_labels=num_classes,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# base_model = base_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "253974ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "base_model = PeftModel.from_pretrained(base_model, \"llama2-ts-finetune-AppliancesEnergy/checkpoint-72/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "245b33b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5811 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################################\n",
      "prediction: 12.596839\n",
      "real: 17.37\n",
      "#######################################################################\n",
      "prediction: 12.286289\n",
      "real: 20.65\n",
      "#######################################################################\n",
      "prediction: 12.406353\n",
      "real: 11.42\n",
      "#######################################################################\n",
      "prediction: 12.456167\n",
      "real: 10.68\n",
      "#######################################################################\n",
      "prediction: 12.288422\n",
      "real: 12.44\n",
      "#######################################################################\n",
      "prediction: 12.834539\n",
      "real: 11.17\n",
      "#######################################################################\n",
      "prediction: 12.408128\n",
      "real: 24.12\n",
      "#######################################################################\n",
      "prediction: 12.46322\n",
      "real: 10.99\n",
      "#######################################################################\n",
      "prediction: 12.478753\n",
      "real: 13.76\n",
      "#######################################################################\n",
      "prediction: 12.59377\n",
      "real: 14.56\n",
      "#######################################################################\n",
      "prediction: 12.578891\n",
      "real: 14.97\n",
      "#######################################################################\n",
      "prediction: 12.608744\n",
      "real: 13.43\n",
      "#######################################################################\n",
      "prediction: 12.564716\n",
      "real: 11.57\n",
      "#######################################################################\n",
      "prediction: 12.682214\n",
      "real: 9.33\n",
      "#######################################################################\n",
      "prediction: 12.619629\n",
      "real: 15.58\n",
      "#######################################################################\n",
      "prediction: 12.187091\n",
      "real: 20.93\n",
      "#######################################################################\n",
      "prediction: 12.322847\n",
      "real: 11.99\n",
      "#######################################################################\n",
      "prediction: 12.47583\n",
      "real: 15.37\n",
      "#######################################################################\n",
      "prediction: 12.331747\n",
      "real: 10.47\n",
      "#######################################################################\n",
      "prediction: 12.272734\n",
      "real: 14.41\n",
      "#######################################################################\n",
      "prediction: 12.548271\n",
      "real: 10.16\n",
      "#######################################################################\n",
      "prediction: 12.402569\n",
      "real: 15.12\n",
      "#######################################################################\n",
      "prediction: 12.413474\n",
      "real: 12.32\n",
      "#######################################################################\n",
      "prediction: 12.372929\n",
      "real: 10.46\n",
      "#######################################################################\n",
      "prediction: 12.777777\n",
      "real: 15.06\n",
      "#######################################################################\n",
      "prediction: 12.130289\n",
      "real: 9.68\n",
      "#######################################################################\n",
      "prediction: 12.333855\n",
      "real: 10.69\n",
      "#######################################################################\n",
      "prediction: 12.410384\n",
      "real: 17.8\n",
      "#######################################################################\n",
      "prediction: 12.509902\n",
      "real: 10.69\n",
      "#######################################################################\n",
      "prediction: 12.223831\n",
      "real: 17.06\n",
      "#######################################################################\n",
      "prediction: 12.493114\n",
      "real: 20.88\n",
      "#######################################################################\n",
      "prediction: 12.111152\n",
      "real: 10.89\n",
      "#######################################################################\n",
      "prediction: 12.523838\n",
      "real: 13.47\n",
      "#######################################################################\n",
      "prediction: 12.421815\n",
      "real: 13.62\n",
      "#######################################################################\n",
      "prediction: 12.417063\n",
      "real: 13.48\n",
      "#######################################################################\n",
      "prediction: 12.713886\n",
      "real: 14.89\n",
      "#######################################################################\n",
      "prediction: 12.253434\n",
      "real: 10.6\n",
      "#######################################################################\n",
      "prediction: 12.462645\n",
      "real: 15.59\n",
      "#######################################################################\n",
      "prediction: 12.295403\n",
      "real: 17.89\n",
      "#######################################################################\n",
      "prediction: 12.561435\n",
      "real: 12.95\n",
      "#######################################################################\n",
      "prediction: 12.752031\n",
      "real: 10.12\n",
      "#######################################################################\n",
      "prediction: 12.626154\n",
      "real: 12.14\n"
     ]
    }
   ],
   "source": [
    "for i_test in range(len(data_TS['test']['text'])):\n",
    "    print('#######################################################################')\n",
    "    model_input_text = eval_tokenizer(data_TS['test']['text'][i_test], return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "    outputs = base_model(input_ids=model_input_text['input_ids'], attention_mask=model_input_text['attention_mask'])\n",
    "    logits = outputs.logits.squeeze()\n",
    "    predictions = logits.detach().cpu().numpy()\n",
    "\n",
    "    print('prediction: ' + str(predictions))\n",
    "    print('real: ' + str(data_TS['test']['labels'][i_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293ffc11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PTBEnv",
   "language": "python",
   "name": "ptbenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}